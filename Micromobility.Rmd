---
title: "Micromobility"
author: "Emma King"
date: "2025-05-07"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(lubridate)
library(sf)
library(basemaps)

```



# Importing county shapefile

The following chunk will import a shapefile of all the counties in Maryland, that has been stored locally. Then, I exclude all other entries besides Montgomery County and overwrite the object named `county`. I transform the CRS to WGS84.

```{r}

county <- st_read("C:/Users/jaria/Downloads/Maryland_Physical_Boundaries_-_County_Boundaries_(Generalized) (1)/Maryland_Physical_Boundaries_-_County_Boundaries_(Generalized).shp")

county <- county[county$county == "Montgomery",]
county <- st_transform(county, crs = 4326)

```



# Importing Capital Bikeshare datasets

The naming convention for tables in Capital Bikeshare's Index Bucket leads with the date in 'yyyymm' format. I create a vector of the years 2019 to 2025 as integers. I create a dataframe using the year vector, replicated by the number of months in a year for each year. Using arithmetic, I combine the year and month columns to achieve the 'yyyymm' format. 

After previewing the files in the index bucket, I find that the data available prior to March 2020 does not include coordinate details. I adjust the scope of the project by limiting the date range to May 2020 to Feb 2025. 

I store the parts of the url that stays consistent for every link, as `a` and `b`, pasting them on their respective ends with the 'yyyymm' integer between them, ultimately creating a vector named `bucket`, which has 58 character strings. I also create a vector with the same information as `bucket`, but in a file location format.

```{r}

# Generating link names 

q <- 2019:2025  
df <- cbind(yy = sort(rep(x=q, 12)), mm = rep(x=1:12, length(q))) |> as.data.frame()
df$yymm <- df$yy*100 + df$mm

# (1) the data for April 2020 is missing from their index; the csv in the zip file for April 2024 is mislabelled as 202004
# (2) the table format changed significantly after March 2020 
#     - most notably, there is no coordinate data available for months prior to May 2020

df <- df[17:(nrow(df)-10), 3]    # changing project date range to May 2020 - Feb 2025

a <- "https://s3.amazonaws.com/capitalbikeshare-data/"
b <- "-capitalbikeshare-tripdata.zip"

bucket <- paste0(a,df,b)
bucket_dir <- paste0("./data/",df,b)
```


I initialize an empty file named 'data' to store the 58 zip files I intend to download. I unzip the first file to extract the csv, save it into the 'data' folder, and read the file into a dataframe named `trips`.

```{r}

# dir.create("./data")
download.file(bucket, bucket_dir, mode = "wb")

file_name <- unzip(bucket_dir[1], exdir = "./data")
trips <- read.csv(file_name[1])
```


I remove entries that lack coordinate data, then convert the `start_lng` and `start_lat` columns into spatial data, making  `trips` an sf object. Using the `county` shapefile, I remove any trips with a start location outside of the county boundaries.
Using a for-loop, I iteratively extract each file listed in `bucket_dir`, read the csv into R, remove entries without coordinate data, convert the data into an sf object, exclude observations outside of Montgomery County, and append the results to the initiated `trips` data frame. I keep track of the progress by printing the value of `i` at the end of each loop. 

I save the results as a csv in the data folder to avoid repeating the process (the for-loop is computationally expensive).

```{r}

# In my original approach, I iteratively unzipped and appended the monthly datasets, but that for-loop took 25 minutes to compile and it produced a dataframe with over 19 million observations. Writing the dataframe to a csv took 15 minutes. 

# My alternative approach includes a for-loop that will likely take a long time to finish, but  will ultimately produce a much smaller dataframe since it will exclude trips that begin outside of Montgomery County (namely, those that begin in DC).


trips <- trips |> filter(!is.na(start_lat))

trips <- st_as_sf(trips, coords = c("start_lng", "start_lat"), remove = FALSE, crs = 4326)

trips <- st_intersection(trips, county$geometry)


for (i in 2:length(bucket)) {
  
  file_name <- unzip(bucket_dir[i], exdir = "./data")
  add_trip <- read.csv(file_name[1])
  
  add_trip <- add_trip |> filter(!is.na(start_lat))
  add_trip <- st_as_sf(add_trip, coords = c("start_lng", "start_lat"), remove = FALSE, crs = 4326)
  add_trip <- st_intersection(add_trip, county$geometry)
  
  trips <- rbind(trips, add_trip)
  
  print(paste0("Progress: ", i, "/58"))
} 


# The for-loop took 35 minutes and trips ended up with 320,701 observations

write.csv(trips,"./trips.csv", row.names = FALSE)

```


```{r}
# Tidying up the environment
rm(a,b,i,q,df,file_name)
```



# Accessing trips.csv

I access a local copy of the trips data / aggregated dataset, which is saved in the current directory. I read the csv into a dataframe and convert it into an sf object.

```{r, warning=FALSE}

trips <- readr::read_csv("trips.csv")

trips <- st_as_sf(trips, coords = c("start_lng", "start_lat"), remove = FALSE, crs = 4326)

```



# Correcting logical fallacies

In the *Importing Capital Bikeshare datasets* section, I excluded any observations that didn't have starting coordinates. Here, I exclude any observations that don't have destination coordinates. 

```{r}

# Remove trips without destination coordinates (-1,057 obs)

trips <- trips |> filter(!is.na(end_lat))

```


I convert the columns detailing the trip's start and end time into date-time format. In this format, I am able to take the difference between these columns and create a duration estimate (in minutes). I remove observations where the trip duration exceeds 24 hours (1440 minutes).

```{r}

# Convert start and end times to POSIXt date-times

trips$started_at <- as.POSIXct(trips$started_at, format = "%Y-%m-%d %H:%M:%S")
trips$ended_at <- as.POSIXct(trips$ended_at, format = "%Y-%m-%d %H:%M:%S")


# Calculate trip duration

trips$duration <- difftime(trips$ended_at, trips$started_at, units = "mins")
trips$duration <- as.numeric(trips$duration)

# nrow(trips[trips$duration > 1440,]) 
trips <- trips |> filter(duration > 0 & duration < 1440)
# nrow(trips[trips$duration > 120,])/nrow(trips)  # ~2%
 
```



# Getting associated metro station

Importing a shapefile containing the locations of all WMATA Metro Stations, which was stored locally, I transform the CRS to WGS84 and remove stations outside of Montgomery County. From `metro_stop`, I create a half-mile buffer around each station, save it as `metro_buff`, and select only the relevant columns. 

I spatially join the trip data (starting location) with the metro buffers, creating columns that either detail the metro region it overlaps or is NA -- not overlapping with any metro region.

```{r}

# Retrieving shapefile

metro_stop <- st_read("C:/Users/jaria/Downloads/Maryland_Transit_-_WMATA_Metro_Stops/Maryland_Transit_-_WMATA_Metro_Stops.shp")
metro_stop <- metro_stop |> 
  st_transform(crs = 4326) |>
  st_crop(county) |>
  filter(MetroLine == "red") |>
  filter((OBJECTID != 32) & (OBJECTID != 84))

metro_buff <- st_buffer(metro_stop, dist = units::set_units(0.5, "miles"))
metro_buff <- metro_buff |> dplyr::select(OBJECTID, NAME, MetroLine, geometry)


# Assigning a metro-buffer region to each trip's starting location

trips <- st_join(trips, metro_buff)

```


I correct the vairable classes for several columns within `trips`, notably changing numeric ID columns into character classes. I also rename the columns added to `trips` from the spatial join with `metro_buff` to specify that the spatial operations were done using the trip's starting location. 

```{r}

# Correcting classes and distinguishing column names

trips <- trips |> 
  mutate(start_station_id = as.character(start_station_id),
         end_station_id = as.character(end_station_id),
         OBJECTID = as.character(OBJECTID)) 

trips <- trips |>
  rename(origin_id = OBJECTID,
         origin_name = NAME,
         origin_line = MetroLine)

```


The for-loop from *Importing Capital Bikeshare datasets* excludes trips with starting locations outside of Montgomery County. Here, I exclude trips with destinations outside of Montgomery County and then do another spatial join with `metro_buff`, this time using trip destination details instead of origin details. I repeat the same clarifying name changes to the columns added from the spatial join.

```{r}

# Excluding trips with destinations outside of Montgomery County (-91,949 obs)

trips <- trips |>
  st_drop_geometry() |>
  st_as_sf(coords = c("end_lng", "end_lat"), remove = FALSE, crs = 4326) |>
  st_intersection(county$geometry)


# Assigning a metro-buffer region to each trip's ending location

trips <- st_join(trips, metro_buff)


# Distinguishing column names

trips <- trips |> 
  mutate(OBJECTID = as.character(OBJECTID)) |>
  rename(dest_id = OBJECTID,
         dest_name = NAME,
         dest_line = MetroLine)

```


I create two variables that broadly classify the trip as starting and/or ending within or outside of a metro buffer region.  

```{r}

# Designate trip origin/destination as being inside/outside metro buffer region

trips <- trips |>
  mutate(origin_metro = ifelse(is.na(origin_id), "outside", "inside"),
         dest_metro = ifelse(is.na(dest_id), "outside", "inside"))

```



# Biking Infrastructure

The following chunk will import the results of the Bicycle Level of Traffic Stress (LTS) Survey by the Montgomery County Planning Department. A zipped geodatabase file will be downloaded directly from the internet, unzipped, and stored in your current directory. From there, R will read the gdb into an sf object named `bike_stress` and transformed to WGS84.

```{r}

# Importing Bicycle Stress Map Survey

download.file("https://mcatlas.org/tiles6/00_Shapefiles/Transportation/Transportation_Master_Plan_Bicycle_Level_of_Traffic_Stress.zip","Transportation_Master_Plan_Bicycle_Level_of_Traffic_Stress.gdb.zip", mode="wb")

unzip("Transportation_Master_Plan_Bicycle_Level_of_Traffic_Stress.gdb.zip", exdir = ".")

bike_stress <- st_read("./Transportation_Master_Plan_Bicycle_Level_of_Traffic_Stress.gdb", 
                       layer = "Bicycle_Level_of_Traffic_Stress_Planned")

bike_stress <- st_transform(bike_stress, crs = 4326)

```


The `bike_stress` object has more than one geometry type, notably including a "MULTICURVE" type, which is not supported by sf. I change the geometry into a "MULTILINESTRING" so that I could perform geometry operations on it. I trim `bike_stress` using `metro_buff` as a stencil, saving the overlapping region in an sf object names `lts`. 

The LTS Survey grades streets using six levels: very low, low, moderately low, moderately high, high, and very high. I consolidate these levels into two: high and low. I calculate the length of each multilinestring, grouping them by metro buffer region and LTS grade. I rearrange the data frame to create High_Stress and Low_Stress columns, that store the calculated street lengths for each stress designation, per metro. Lastly, I determine what proportion of the total street length within a metro buffer is designated as high stress, storing it in the object `metro_lts`.  

```{r}

# Trimming the shapefile to exclude streets outside of a metro's half-mile buffer region

bike_stress <- st_cast(bike_stress, "MULTILINESTRING")

lts <- st_intersection(bike_stress, metro_buff)  


# Get length of all streets within the buffer, then see what proportion of the total length is high stress

lts <- lts |>
  mutate(LTS = ifelse(grepl("High", LTS_TEXT), "High_Stress", "Low_Stress"),
         LENGTH = st_length(Shape)) |>
  group_by(NAME, LTS) |>
  summarise(STREET_LENGTH = sum(LENGTH)) 

metro_lts <- lts |>
  st_drop_geometry() |>
  spread(key = LTS, value = STREET_LENGTH) |>
  mutate(PROP_LTS = High_Stress / (High_Stress + Low_Stress)) 
  

```


I plot the portion of streets that overlap a metro buffer, coloring it according to its LTS designation. Due to the drawing order, the high stress streets are plotted underneath the low stress streets. To make it more visible, I plot the high stress streets independently in the following geom_sf layer. 

```{r}

# Mapping street stress in buffered regions

ggplot() +
  geom_sf(data = lts, aes(color = LTS)) +
  geom_sf(data = lts[lts$LTS=="High_Stress",], color = "red") +
  scale_color_discrete(name = "Level of Traffic Stress", labels = c("High Stress", "Low Stress")) +
  coord_sf() +
  theme_light()

```


Returning to the central dataset, `trips`, I group all trips by their origin metro station and determine the total number of trips taken at each metro from 2020-2025. I join this data with the `metro_lts` data, which contains the proportion of streets that are high stress for each metro region. Excluding trips that were not initiated near a metro station, I create a scatterplot that compares the proportion of high stress streets against the total number of trips initiated per station, followed by a correlation test. The scatterplot displays two points with particularly large number of trips (Silver Spring and Bethesda). Removing these two potential outliers and performing another correlation test, I find that the relationship between PROP_LTS and N_TRIPS is not significant in either scenario. 

```{r}

# Assessing relationship between traffic stress and trip frequency in a region

metro_lts <- trips |>
  st_drop_geometry() |>
  group_by(origin_name) |>
  summarise(N_TRIPS = n()) |>
  rename(NAME = origin_name) |>
  full_join(metro_lts, by = "NAME") |>
  units::drop_units() |>
  filter(!is.na(NAME))

metro_lts |>
  ggplot() +
  geom_point(aes(x = PROP_LTS, y = N_TRIPS), size = 2) +
  theme_light() +
  labs(x = "Proportion of Street Length with High Traffic Stress",
       y = "Total Number of Trips")


# Correlation test between number of trips and traffic stress 

cor.test(metro_lts$N_TRIPS, metro_lts$PROP_LTS, method = "pearson")

cor.test(metro_lts$N_TRIPS[metro_lts$N_TRIPS < 20000], metro_lts$PROP_LTS[metro_lts$N_TRIPS < 20000], method = "pearson")

```



# Date-time and Duration Histograms

The following chunk produces a histogram that plots the number of trips taken from 2020 to 2025, distinguishing membership by color. The number of bins is equal to the number of months in our data's date range (May 2020 - February 2025). A density line is plotted atop the histogram after being scaled to size. 

```{r}

# Graphing the seasonal and perennial distribution of trip frequency

trips |>
  ggplot(aes(x = started_at)) +
  geom_histogram(aes(fill = member_casual), color = "white", position = "identity", bins = 56, alpha = 0.1) +
  geom_density(aes(color = member_casual, y = after_stat(count)*3333333), linewidth = 1) +  
  theme_light() +
  labs(x = "Date", y = "Count", fill = "Membership", color = "Membership")

```


In the following graph, trip characteristics are assessed with respect to metro proximity. I create more descriptive labels for the graph facets, explicitly stating details about trip origin and destination. The `trips` dataset is truncated to only include trips with a duration under 20 minutes because this represents over 70% of all trips taken.

```{r}


dest_labs <- c("Destination Near Metro", "Destination Far From Metro")
names(dest_labs) <- c("inside", "outside")

origin_labs <- c("Initiated Near Metro", "Initiated Far From Metro")
names(origin_labs) <- c("inside", "outside")

ggplot() +
  geom_histogram(data = trips[trips$duration < 20,], aes(x = duration), bins = 30) +
  facet_grid(dest_metro ~ origin_metro, labeller = labeller(dest_metro = dest_labs, origin_metro = origin_labs)) +
  theme_bw() +
  labs(y = "Count", x = "Trip Duration (minutes)")
  

```



# Formatting basemap and labels

Using the 'basemap' package, the default basemap style is set to light colored map produced by Carto. The map extent was selected to fit the metro buffers with their labels, which stretched the map laterally. Without adjustment, the map labels overlap the buffer region it names. To offset the label text, the position of each label was arranged individually.

The basemap was plotted first, followed by geom_sf layers for the metro stations, their buffers, and their labels. To be plotted with the basemap, the sf objects had to transformed to the CRS 3857, which is used for web mapping.

```{r, warning=FALSE}

# Establishing basemap

set_defaults(map_service = "carto", map_type = "light") 


# Setting map extent

# bb_metro <- st_as_sfc(st_bbox(metro_buff))
# bb_metro <- st_buffer(bb_metro, dist = units::set_units(1, "miles"))
bb_metro <- st_as_sfc(st_bbox(c(xmin=-77.26925, ymin=38.93634, xmax=-76.91533, ymax=39.14307), crs = 4326))


# Dictating label positions

metro_buff$nudge_x <- -4000 
metro_buff$nudge_x[metro_buff$NAME %in% c("Glenmont", "Wheaton", "Forest Glen", "Silver Spring", "Takoma")] <- 4000

metro_buff$nudge_x[metro_buff$NAME == "Shady Grove"] <- -5200
metro_buff$nudge_x[metro_buff$NAME == "Twinbrook"] <- -4400
metro_buff$nudge_x[metro_buff$NAME == "Medical Center"] <- -5600
metro_buff$nudge_x[metro_buff$NAME == "Grosvenor-Strathmore"] <- -7600
metro_buff$nudge_x[metro_buff$NAME == "Friendship Heights"] <- -6500
metro_buff$nudge_x[metro_buff$NAME == "Forest Glen"] <- 4800
metro_buff$nudge_x[metro_buff$NAME == "Silver Spring"] <- 5200
metro_buff$nudge_x[metro_buff$NAME == "White Flint"] <- -4500
metro_buff$nudge_x[metro_buff$NAME == "Glenmont"] <- 4200


# Map of metro buffer regions

ggplot() +
  basemap_gglayer(bb_metro) +
  scale_fill_identity() + 
  geom_sf(data = st_transform(metro_buff, crs = 3857), fill = NA) +
  geom_sf(data = st_transform(metro_stop, crs = 3857), size=2) +
  geom_sf_text(data = st_transform(metro_buff, crs = 3857), aes(label = NAME), nudge_x = metro_buff$nudge_x) +
  coord_sf() +
  theme_light() +
  theme(legend.position = "none")


```





# ACS Choropleth

From the 2023 American Community Survey, I have downloaded the Means of Transportation to Work table for census block-groups in Montgomery County, storing it locally. Using the csv with column-metadata, I selected the most relevant columns and renamed them (from abstract names to descriptive ones), saving it as `commute`.

From Maryland iMap, I downloaded a shapefile (storing it locally) with the boundaries for all of the census block-groups in Montgomery County, naming it `blocks`. To merge with the `commute` data, I alter GEOID20 (the unique identifier for each census block-group) to match the geo_id format observed in `commute`.

```{r}

# Importing Means of Transportation to Work survey by ACS for 2023 (the most recent)

commute <- read_csv("C:/Users/jaria/Downloads/ACSDT5Y2023.B08301_2025-04-25T114226/ACSDT5Y2023.B08301-Data.csv")

commute <- commute |>
  dplyr::select(
    "geo_id" = GEO_ID, 
    "total" = B08301_001E, 
    "total_err" = B08301_001M,  
    "car" = B08301_002E, 
    "car_err" = B08301_002M, 
    "public" = B08301_010E, 
    "public_err" = B08301_010M, 
    "bike" = B08301_018E, 
    "bike_err" = B08301_018M, 
    "home" = B08301_021E, 
    "home_err" = B08301_021M
  )


# Importing census block-group boundaries and merging with commute data

blocks <- st_read("C:/Users/jaria/Downloads/Maryland_Census_Boundaries_-_Census_Block_Groups_2020/Maryland_Census_Boundaries_-_Census_Block_Groups_2020.shp")

blocks <- blocks |>
  dplyr::select("geo_id" = GEOID20) |>
  mutate(geo_id = gsub("^*", "1500000US", geo_id)) |>
  right_join(commute, by = "geo_id")

```


After coercing the columns into their correct data type (numeric), I calculate the total population of commuters (excluding those who work from home) and determine the proportion of each block-group's population that uses cars. Using this metric, I create a choropleth, centering the extent over the metro regions in Montgomery County.

```{r}

# Means of Transportation (2023) Choropleth 

blocks |>
  mutate(total = as.numeric(total),
         home = as.numeric(home),
         car = as.numeric(car),
         public = as.numeric(public),
         bike = as.numeric(bike)) |>
  mutate(total = total - home) |>
  mutate(car_prop = car/total,
         public_prop = public/total,
         bike_prop = bike/total) |>
  st_crop(st_transform(bb_metro, crs = 3857)) |>
  ggplot() +
  geom_sf(aes(fill = car_prop), color = "white") +
  scale_fill_distiller(palette = "GnBu") +
  geom_sf(data = metro_buff, fill = NA, color = "black", alpha = 0.5) +
  coord_sf() +
  theme_minimal() +
  labs(fill = "Proportion of \nCar Commuters")

```



# Yearly Trip Counts

I group trips by origin station and year to determine the number of trips initiated at each station for the years between 2020 and 2025. After getting the yearly trip count per station, I calculate their annual averages, storing the results in `annual_trips`.

To place the annual averages in their spatial context, I join `annual_trips` and `metro_buff`. I first plot the basemap using the extent determined in *Formatting basemap and labels*, which uses the function 'scale_fill_identity.' To add another geom layer that also uses a scale_fill aesthetic, I use the function 'new_scale_fill' from the 'ggnewscale' package. After distinguishing the layers that display a scale_fill aesthetic, I color the metro buffer regions according to their average annual number of trips. I finish the plot by adding the map labels. 

```{r}

# Average annual number of trips per metro buffer region (mapped) 

annual_trips <- trips |>
  st_drop_geometry() |>
  mutate(year = year(started_at)) |>
  group_by(origin_name, year) |>
  summarise(n_trips = n()) |>
  group_by(origin_name) |>
  summarise(avg_annual = mean(n_trips)) |>
  filter(!is.na(origin_name)) |>
  as.data.frame() |>
  right_join(metro_buff, by = c("origin_name" = "NAME")) |>
  st_as_sf()


ggplot() +
  basemap_gglayer(bb_metro) +
  scale_fill_identity() +
  ggnewscale::new_scale_fill() +
  geom_sf(data = st_transform(annual_trips, crs = 3857), aes(fill = avg_annual), alpha = 0.8) +
  scale_fill_distiller(palette = "Spectral", name = "Average Annual \nNumber of Trips") +
  geom_sf(data = st_transform(metro_stop, crs = 3857), size=1) +
  geom_sf_text(data = st_transform(metro_buff, crs = 3857), aes(label = NAME), nudge_x = metro_buff$nudge_x) +
  coord_sf() +
  theme_minimal()

```


To depict the scope of change in the annual total number of trips, I group trips by station and year, then create columns for each year with their respective annual trip count. I calculate the growth rate of consecutive years and store this information as `trip_growth`.

```{r}

# Growth rate of annual number of trips per metro buffer region 

trip_growth <- trips |>
  st_drop_geometry() |>
  mutate(year = year(started_at)) |>
  group_by(origin_name, year) |>
  summarise(n_trips = n()) |>
  mutate(year = as.character(year)) |>
  mutate(year = gsub("^*", "Y", year)) |>
  spread(key = year, value = n_trips) |>
  filter(!is.na(origin_name)) |>
  mutate(grow24 = (Y2024-Y2023)/Y2023,
         grow23 = (Y2023-Y2022)/Y2022,
         grow22 = (Y2022-Y2021)/Y2021) |>
  as.data.frame() 

```


The annual trip growth rate per station is plotted as grouped bar chart.

```{r}

trip_growth |>
  gather(key = "growth_year", value = "growth_rate", grow24:grow22) |>
  ggplot() +
  geom_bar(aes(x = origin_name, y = growth_rate, fill = growth_year), stat = "identity", position = "dodge", width = 0.65) +
  theme_light() +
  labs(x = "Origin Metro Station", y = "Growth Rate of Annual Number of Trips") +
  scale_fill_discrete(name = "Year", labels = c("2022", "2023", "2024"))

```


The most recent data available at the start of this project from the Capital Bikeshare is from February 2025. To compare the trip frequency for 2025 to other years, for each station, I get the total number of trips from the month of February. The results are plotted in a grouped bar graph, where bar color denotes year.

```{r}

# Total Trips in February by Year, per Station (2021-2025)

trips |>
  st_drop_geometry() |>
  mutate(month = month(started_at),
         year = year(started_at)) |>
  filter(month == 2) |>
  group_by(origin_name, year) |>
  summarise(n_trips = n()) |>
  filter(!is.na(origin_name)) |>
  ggplot() +
  geom_bar(aes(x = origin_name, y = n_trips, fill = as.factor(year)), 
           stat = "identity", position = "dodge", color = "black", width = 0.7) +
  labs(x = "Origin Metro Station", y = "Total Number of Trips in February") +
  scale_fill_discrete(name = "Year") +
  theme_minimal()
  

```


To get a time series graph for each station, I calculate the monthly total of trips for each metro. Each line is colored according to year, excluding any years without data for all 12 months. The graph facets are ordered in coordination with the stations' spatial arrangement, which can be observed in any of the previously generated maps.

```{r}

# Time Series (faceted)

trips |>
  st_drop_geometry() |>
  mutate(year = year(started_at),
         month = month(started_at)) |>
  group_by(origin_name, month, year) |>
  summarise(n_trips = n()) |>
  filter(!is.na(origin_name),
         (year < 2025) & (year > 2020)) |>
  ggplot() +
  geom_line(aes(x = month, y = n_trips, color = as.factor(year))) +
  facet_wrap(~ factor(origin_name, levels = c('Shady Grove', 'Rockville', 'Twinbrook', 'White Flint', 'Grosvenor-Strathmore',
                                              'Medical Center', 'Bethesda', 'Friendship Heights', 'Glenmont', 'Wheaton',
                                              'Forest Glen', 'Silver Spring', 'Takoma')), 
             scales = "free", nrow = 2, ncol = 8) +
  theme_minimal() +
  labs(x = "Month", y = "Monthly Total Number of Trips") +
  scale_color_discrete(name = "Year")

```



# Boxplots and t-tests

Grouping by week, year, and membership, I get the weekly total number of trips by members and by casual users. The results are saved as `wk_member` and illustrated in a box plot. The graph is followed by a t-test, which proves to be significant.

```{r}

wk_member <- trips |>
  st_drop_geometry() |>
  mutate(year = year(started_at),
         week = week(started_at)) |>
  group_by(member_casual, week, year) |>
  summarise(n_trips = n()) 


ggplot(data = wk_member, aes(x = member_casual, y = n_trips)) +
  stat_boxplot(geom = "errorbar", width = 0.2) +
  geom_boxplot(aes(fill = member_casual)) +
  scale_fill_hue(l = 90, c = 30) +
  labs(x = "Membership Status", y = "Weekly Total Number of Trips", fill = "Membership") +
  theme_light()


t.test(n_trips ~ member_casual, data = wk_member)
  
```


Grouping by week, year, and proximity of destination to a metro region (binary), I get the weekly total number of trips that end near a station and those that end far from one. The results are saved as `wk_dest` and illustrated in a box plot. The graph is followed by a t-test, which proves to be significant.

```{r}

wk_dest <- trips |>
  st_drop_geometry() |>
  mutate(year = year(started_at),
         week = week(started_at)) |>
  group_by(dest_metro, week, year) |>
  summarise(n_trips = n()) 


ggplot(data = wk_dest, aes(x = dest_metro, y = n_trips)) +
  stat_boxplot(geom = "errorbar", width = 0.2) +
  geom_boxplot(aes(fill = dest_metro)) +
  scale_fill_hue(l = 90, c = 30) +
  labs(x = "Trip Destination, relative to Metro Station Radius", y = "Weekly Total Number of Trips", fill = "Destination") +
  theme_light() 


t.test(n_trips ~ dest_metro, data = wk_dest)

```


Grouping by week, year, and proximity of origin to a metro region (binary), I get the weekly total number of trips that begin near a station and those that begin far from one. The results are saved as `wk_origin` and illustrated in a box plot. The graph is followed by a t-test, which proves to be significant.

```{r}

wk_origin <- trips |>
  st_drop_geometry() |>
  mutate(year = year(started_at),
         week = week(started_at)) |>
  group_by(origin_metro, week, year) |>
  summarise(n_trips = n()) 


ggplot(data = wk_origin, aes(x = origin_metro, y = n_trips)) +
  stat_boxplot(geom = "errorbar", width = 0.2) +
  geom_boxplot(aes(fill = origin_metro)) +
  scale_fill_hue(l = 90, c = 30) +
  labs(x = "Trip Origin, relative to Metro Station Radius", y = "Weekly Total Number of Trips", fill = "Origin") +
  theme_light()


t.test(n_trips ~ origin_metro, data = wk_origin)

```






