---
title: "Micromobility"
author: "Emma King"
date: "2025-05-07"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(lubridate)
library(sf)
library(basemaps)

```



# Importing county shapefile

The following chunk will import a shapefile of all the counties in Maryland, that has been stored locally. Then, I exclude all other entries besides Montgomery County and overwrite the object named `county`. I transform the CRS to WGS84.

```{r}

county <- st_read("C:/Users/jaria/Downloads/Maryland_Physical_Boundaries_-_County_Boundaries_(Generalized) (1)/Maryland_Physical_Boundaries_-_County_Boundaries_(Generalized).shp")

county <- county[county$county == "Montgomery",]
county <- st_transform(county, crs = 4326)

```



# Importing Capital Bikeshare datasets

The naming convention for tables in Capital Bikeshare's Index Bucket leads with the date in 'yyyymm' format. I create a vector of the years 2019 to 2025 as integers. I create a dataframe using the year vector, replicated by the number of months in a year for each year. Using arithmetic, I combine the year and month columns to achieve the 'yyyymm' format. 

After previewing the files in the index bucket, I find that the data available prior to March 2020 does not include coordinate details. I adjust the scope of the project by limiting the date range to May 2020 to Feb 2025. 

I store the parts of the url that stays consistent for every link, as `a` and `b`, pasting them on their respective ends with the 'yyyymm' integer between them, ultimately creating a vector named `bucket`, which has 58 character strings. I also create a vector with the same information as `bucket`, but in a file location format.

```{r}

# Generating link names 

q <- 2019:2025  
df <- cbind(yy = sort(rep(x=q, 12)), mm = rep(x=1:12, length(q))) |> as.data.frame()
df$yymm <- df$yy*100 + df$mm

# (1) the data for April 2020 is missing from their index; the csv in the zip file for April 2024 is mislabelled as 202004
# (2) the table format changed significantly after March 2020 
#     - most notably, there is no coordinate data available for months prior to May 2020

df <- df[17:(nrow(df)-10), 3]    # changing project date range to May 2020 - Feb 2025

a <- "https://s3.amazonaws.com/capitalbikeshare-data/"
b <- "-capitalbikeshare-tripdata.zip"

bucket <- paste0(a,df,b)
bucket_dir <- paste0("./data/",df,b)
```


I initialize an empty file named 'data' to store the 58 zip files I intend to download. I unzip the first file to extract the csv, save it into the 'data' folder, and read the file into a dataframe named `trips`.

```{r}

# dir.create("./data")
download.file(bucket, bucket_dir, mode = "wb")

file_name <- unzip(bucket_dir[1], exdir = "./data")
trips <- read.csv(file_name[1])
```


I remove entries that lack coordinate data, then convert the `start_lng` and `start_lat` columns into spatial data, making  `trips` an sf object. Using the `county` shapefile, I remove any trips with a start location outside of the county boundaries.
Using a for-loop, I iteratively extract each file listed in `bucket_dir`, read the csv into R, remove entries without coordinate data, convert the data into an sf object, exclude observations outside of Montgomery County, and append the results to the initiated `trips` data frame. I keep track of the progress by printing the value of `i` at the end of each loop. 

I save the results as a csv in the data folder to avoid repeating the process (the for-loop is computationally expensive).

```{r}

# In my original approach, I iteratively unzipped and appended the monthly datasets, but that for-loop took 25 minutes to compile and it produced a dataframe with over 19 million observations. Writing the dataframe to a csv took 15 minutes. 

# My alternative approach includes a for-loop that will likely take a long time to finish, but  will ultimately produce a much smaller dataframe since it will exclude trips that begin outside of Montgomery County (namely, those that begin in DC).


trips <- trips |> filter(!is.na(start_lat))

trips <- st_as_sf(trips, coords = c("start_lng", "start_lat"), remove = FALSE, crs = 4326)

trips <- st_intersection(trips, county$geometry)


for (i in 2:length(bucket)) {
  
  file_name <- unzip(bucket_dir[i], exdir = "./data")
  add_trip <- read.csv(file_name[1])
  
  add_trip <- add_trip |> filter(!is.na(start_lat))
  add_trip <- st_as_sf(add_trip, coords = c("start_lng", "start_lat"), remove = FALSE, crs = 4326)
  add_trip <- st_intersection(add_trip, county$geometry)
  
  trips <- rbind(trips, add_trip)
  
  print(paste0("Progress: ", i, "/58"))
} 


# The for-loop took 35 minutes and trips ended up with 320,701 observations

write.csv(trips,"./trips.csv", row.names = FALSE)

```


```{r}
# Tidying up the environment
rm(a,b,i,q,df,file_name)
```



# Accessing trips.csv

I access a local copy of the trips data / aggregated dataset, which is saved in the current directory. I read the csv into a dataframe and convert it into an sf object.

```{r, warning=FALSE}

trips <- readr::read_csv("trips.csv")

trips <- st_as_sf(trips, coords = c("start_lng", "start_lat"), remove = FALSE, crs = 4326)

```



# Correcting logical fallacies

In the *Importing Capital Bikeshare datasets* section, I excluded any observations that didn't have starting coordinates. Here, I exclude any observations that don't have destination coordinates. 

```{r}

# Remove trips without destination coordinates (-1,057 obs)

trips <- trips |> filter(!is.na(end_lat))

```


I convert the columns detailing the trip's start and end time into date-time format. In this format, I am able to take the difference between these columns and create a duration estimate (in minutes). I remove observations where the trip duration exceeds 24 hours (1440 minutes).

```{r}

# Convert start and end times to POSIXt date-times

trips$started_at <- as.POSIXct(trips$started_at, format = "%Y-%m-%d %H:%M:%S")
trips$ended_at <- as.POSIXct(trips$ended_at, format = "%Y-%m-%d %H:%M:%S")


# Calculate trip duration

trips$duration <- difftime(trips$ended_at, trips$started_at, units = "mins")
trips$duration <- as.numeric(trips$duration)

# nrow(trips[trips$duration > 1440,]) 
trips <- trips |> filter(duration > 0 & duration < 1440)
# nrow(trips[trips$duration > 120,])/nrow(trips)  # ~2%
 
```



# Getting associated metro station

Importing a shapefile containing the locations of all WMATA Metro Stations, which was stored locally, I transform the CRS to WGS84 and remove stations outside of Montgomery County. From `metro_stop`, I create a half-mile buffer around each station, save it as `metro_buff`, and select only the relevant columns. 

I spatially join the trip data (starting location) with the metro buffers, creating columns that either detail the metro region it overlaps or is NA -- not overlapping with any metro region.

```{r}

# Retrieving shapefile

metro_stop <- st_read("C:/Users/jaria/Downloads/Maryland_Transit_-_WMATA_Metro_Stops/Maryland_Transit_-_WMATA_Metro_Stops.shp")
metro_stop <- metro_stop |> 
  st_transform(crs = 4326) |>
  st_crop(county) |>
  filter(MetroLine == "red") |>
  filter((OBJECTID != 32) & (OBJECTID != 84))

metro_buff <- st_buffer(metro_stop, dist = units::set_units(0.5, "miles"))
metro_buff <- metro_buff |> dplyr::select(OBJECTID, NAME, MetroLine, geometry)


# Assigning a metro-buffer region to each trip's starting location

trips <- st_join(trips, metro_buff)

```


I correct the vairable classes for several columns within `trips`, notably changing numeric ID columns into character classes. I also rename the columns added to `trips` from the spatial join with `metro_buff` to specify that the spatial operations were done using the trip's starting location. 

```{r}

# Correcting classes and distinguishing column names

trips <- trips |> 
  mutate(start_station_id = as.character(start_station_id),
         end_station_id = as.character(end_station_id),
         OBJECTID = as.character(OBJECTID)) 

trips <- trips |>
  rename(origin_id = OBJECTID,
         origin_name = NAME,
         origin_line = MetroLine)

```


The for-loop from *Importing Capital Bikeshare datasets* excludes trips with starting locations outside of Montgomery County. Here, I exclude trips with destinations outside of Montgomery County and then do another spatial join with `metro_buff`, this time using trip destination details instead of origin details. I repeat the same clarifying name changes to the columns added from the spatial join.

```{r}

# Excluding trips with destinations outside of Montgomery County (-91,949 obs)

trips <- trips |>
  st_drop_geometry() |>
  st_as_sf(coords = c("end_lng", "end_lat"), remove = FALSE, crs = 4326) |>
  st_intersection(county$geometry)


# Assigning a metro-buffer region to each trip's ending location

trips <- st_join(trips, metro_buff)


# Distinguishing column names

trips <- trips |> 
  mutate(OBJECTID = as.character(OBJECTID)) |>
  rename(dest_id = OBJECTID,
         dest_name = NAME,
         dest_line = MetroLine)

```


I create two variables that broadly classify the trip as starting and/or ending within or outside of a metro buffer region.  

```{r}

# Designate trip origin/destination as being inside/outside metro buffer region

trips <- trips |>
  mutate(origin_metro = ifelse(is.na(origin_id), "outside", "inside"),
         dest_metro = ifelse(is.na(dest_id), "outside", "inside"))

```



# Biking Infrastructure

```{r}

# Importing Bicycle Stress Map Survey

download.file("https://mcatlas.org/tiles6/00_Shapefiles/Transportation/Transportation_Master_Plan_Bicycle_Level_of_Traffic_Stress.zip","Transportation_Master_Plan_Bicycle_Level_of_Traffic_Stress.gdb.zip", mode="wb")

unzip("Transportation_Master_Plan_Bicycle_Level_of_Traffic_Stress.gdb.zip", exdir = ".")

bike_stress <- st_read("./Transportation_Master_Plan_Bicycle_Level_of_Traffic_Stress.gdb", 
                       layer = "Bicycle_Level_of_Traffic_Stress_Planned")

bike_stress <- st_transform(bike_stress, crs = 4326)

```


```{r}

# Trimming the shapefile to exclude streets outside of a metro's half-mile buffer region

bike_stress <- st_cast(bike_stress, "MULTILINESTRING")

lts <- st_intersection(bike_stress, metro_buff)  


# Get length of all streets within the buffer, then see what proportion of the total length is high stress

lts <- lts |>
  mutate(LTS = ifelse(grepl("High", LTS_TEXT), "High_Stress", "Low_Stress"),
         LENGTH = st_length(Shape)) |>
  group_by(NAME, LTS) |>
  summarise(STREET_LENGTH = sum(LENGTH)) 

metro_lts <- lts |>
  st_drop_geometry() |>
  spread(key = LTS, value = STREET_LENGTH) |>
  mutate(PROP_LTS = High_Stress / (High_Stress + Low_Stress)) 
  

```

```{r}

# Mapping street stress in buffered regions

ggplot() +
  geom_sf(data = st_transform(lts, crs = 3857), aes(color = LTS)) +
  geom_sf(data = st_transform(lts[lts$LTS=="High_Stress",], crs = 3857), color = "red") +
  scale_color_discrete(name = "Level of Traffic Stress", labels = c("High Stress", "Low Stress")) +
  coord_sf() +
  theme_light()

```


```{r}

# Assessing relationship between traffic stress and trip frequency in a region

metro_lts <- trips |>
  st_drop_geometry() |>
  group_by(origin_name) |>
  summarise(N_TRIPS = n()) |>
  rename(NAME = origin_name) |>
  full_join(metro_lts, by = "NAME") |>
  units::drop_units() |>
  filter(!is.na(NAME))

metro_lts |>
  ggplot() +
  geom_point(aes(x = PROP_LTS, y = N_TRIPS), size = 2) +
  theme_light() +
  labs(x = "Proportion of Street Length with High Traffic Stress",
       y = "Total Number of Trips")


# Correlation test between number of trips and traffic stress 

cor.test(metro_lts$N_TRIPS, metro_lts$PROP_LTS, method = "pearson")

cor.test(metro_lts$N_TRIPS[metro_lts$N_TRIPS < 20000], metro_lts$PROP_LTS[metro_lts$N_TRIPS < 20000], method = "pearson")

```




# Adding date-time details

```{r}

# Graphing the seasonal and perennial distribution of trip frequency

trips |>
  ggplot(aes(x = started_at)) +
  geom_histogram(aes(fill = member_casual), color = "white", position = "identity", bins = 56, alpha = 0.1) +
  geom_density(aes(color = member_casual, y = after_stat(count)*3333333), linewidth = 1) +  
  theme_light() +
  labs(x = "Date", y = "Count", fill = "Membership", color = "Membership")

```





# Making maps

```{r, warning=FALSE}

# Establishing basemap

set_defaults(map_service = "carto", map_type = "light") 


# Setting map extent

# bb_metro <- st_as_sfc(st_bbox(metro_buff))
# bb_metro <- st_buffer(bb_metro, dist = units::set_units(1, "miles"))
bb_metro <- st_as_sfc(st_bbox(c(xmin=-77.26925, ymin=38.93634, xmax=-76.91533, ymax=39.14307), crs = 4326))


# Dictating label positions

metro_buff$nudge_x <- -4000 
metro_buff$nudge_x[metro_buff$NAME %in% c("Glenmont", "Wheaton", "Forest Glen", "Silver Spring", "Takoma")] <- 4000

metro_buff$nudge_x[metro_buff$NAME == "Shady Grove"] <- -5200
metro_buff$nudge_x[metro_buff$NAME == "Twinbrook"] <- -4400
metro_buff$nudge_x[metro_buff$NAME == "Medical Center"] <- -5600
metro_buff$nudge_x[metro_buff$NAME == "Grosvenor-Strathmore"] <- -7600
metro_buff$nudge_x[metro_buff$NAME == "Friendship Heights"] <- -6500
metro_buff$nudge_x[metro_buff$NAME == "Forest Glen"] <- 4800
metro_buff$nudge_x[metro_buff$NAME == "Silver Spring"] <- 5200
metro_buff$nudge_x[metro_buff$NAME == "White Flint"] <- -4500
metro_buff$nudge_x[metro_buff$NAME == "Glenmont"] <- 4200


# Map of metro buffer regions

ggplot() +
  basemap_gglayer(bb_metro) +
  scale_fill_identity() + 
  # geom_sf(data = metro.line, aes(color = OBJECTID_1), linewidth = 2) +
  # scale_color_manual(values = c("orangered3", "gold", "chartreuse3", "gold", "gray50")) +
  # geom_sf(data = st_transform(trips, crs = 3857), aes(color = year(started_at)), alpha = 0.4) +
  geom_sf(data = st_transform(metro_buff, crs = 3857), fill = NA) +
  geom_sf(data = st_transform(metro_stop, crs = 3857), size=2) +
  geom_sf_text(data = st_transform(metro_buff, crs = 3857), aes(label = NAME), nudge_x = metro_buff$nudge_x) +
  # geom_sf_text(data = st_transform(metro_buff, crs = 3857), aes(label = NAME)) +
  coord_sf() +
  theme_light() +
  theme(legend.position = "none")


```




# ACS Census

```{r}

# Importing Means of Transportation to Work survey by ACS for 2023 (the most recent)

commute <- read_csv("C:/Users/jaria/Downloads/ACSDT5Y2023.B08301_2025-04-25T114226/ACSDT5Y2023.B08301-Data.csv")

commute <- commute |>
  dplyr::select(
    "geo_id" = GEO_ID, 
    "total" = B08301_001E, 
    "total_err" = B08301_001M,  
    "car" = B08301_002E, 
    "car_err" = B08301_002M, 
    "public" = B08301_010E, 
    "public_err" = B08301_010M, 
    "bike" = B08301_018E, 
    "bike_err" = B08301_018M, 
    "home" = B08301_021E, 
    "home_err" = B08301_021M
  )


# Importing census block-group boundaries and merging with commute data

blocks <- st_read("C:/Users/jaria/Downloads/Maryland_Census_Boundaries_-_Census_Block_Groups_2020/Maryland_Census_Boundaries_-_Census_Block_Groups_2020.shp")

blocks <- blocks |>
  dplyr::select("geo_id" = GEOID20) |>
  mutate(geo_id = gsub("^*", "1500000US", geo_id)) |>
  right_join(commute, by = "geo_id")

```


```{r}

# Means of Transportation (2023) Choropleth 

blocks |>
  mutate(total = as.numeric(total),
         home = as.numeric(home),
         car = as.numeric(car),
         public = as.numeric(public),
         bike = as.numeric(bike)) |>
  mutate(total = total - home) |>
  mutate(car_prop = car/total,
         public_prop = public/total,
         bike_prop = bike/total) |>
  st_crop(st_transform(bb_metro, crs = 3857)) |>
  ggplot() +
  geom_sf(aes(fill = car_prop), color = "white") +
  scale_fill_distiller(palette = "GnBu") +
  geom_sf(data = metro_buff, fill = NA, color = "black", alpha = 0.5) +
  coord_sf() +
  theme_minimal() +
  labs(fill = "Proportion of \nCar Commuters")

```




# Next

```{r}

# Average annual number of trips per metro buffer region (mapped) 

annual_trips <- trips |>
  st_drop_geometry() |>
  mutate(year = year(started_at)) |>
  group_by(origin_name, year) |>
  summarise(n_trips = n()) |>
  group_by(origin_name) |>
  summarise(avg_annual = mean(n_trips)) |>
  filter(!is.na(origin_name)) |>
  as.data.frame() |>
  right_join(metro_buff, by = c("origin_name" = "NAME")) |>
  st_as_sf()


ggplot() +
  basemap_gglayer(bb_metro) +
  scale_fill_identity() +
  ggnewscale::new_scale_fill() +
  geom_sf(data = st_transform(annual_trips, crs = 3857), aes(fill = avg_annual), alpha = 0.8) +
  scale_fill_distiller(palette = "Spectral", name = "Average Annual \nNumber of Trips") +
  geom_sf(data = st_transform(metro_stop, crs = 3857), size=1) +
  geom_sf_text(data = st_transform(metro_buff, crs = 3857), aes(label = NAME), nudge_x = metro_buff$nudge_x) +
  coord_sf() +
  theme_minimal()

```


```{r}

# Growth rate of annual number of trips per metro buffer region 

trip_growth <- trips |>
  st_drop_geometry() |>
  mutate(year = year(started_at)) |>
  group_by(origin_name, year) |>
  summarise(n_trips = n()) |>
  mutate(year = as.character(year)) |>
  mutate(year = gsub("^*", "Y", year)) |>
  spread(key = year, value = n_trips) |>
  filter(!is.na(origin_name)) |>
  mutate(grow24 = (Y2024-Y2023)/Y2023,
         grow23 = (Y2023-Y2022)/Y2022,
         grow22 = (Y2022-Y2021)/Y2021) |>
  as.data.frame() 

```


```{r}

trip_growth |>
  gather(key = "growth_year", value = "growth_rate", grow24:grow22) |>
  ggplot() +
  geom_bar(aes(x = origin_name, y = growth_rate, fill = growth_year), stat = "identity", position = "dodge", width = 0.65) +
  theme_light() +
  labs(x = "Origin Metro Station", y = "Growth Rate of Annual Number of Trips") +
  scale_fill_discrete(name = "Year", labels = c("2022", "2023", "2024"))

```


```{r}

# Total Trips in February by Year, per Station (2021-2025)

trips |>
  st_drop_geometry() |>
  mutate(month = month(started_at),
         year = year(started_at)) |>
  filter(month == 2) |>
  group_by(origin_name, year) |>
  summarise(n_trips = n()) |>
  filter(!is.na(origin_name)) |>
  ggplot() +
  geom_bar(aes(x = origin_name, y = n_trips, fill = as.factor(year)), 
           stat = "identity", position = "dodge", color = "black", width = 0.7) +
  labs(x = "Origin Metro Station", y = "Total Number of Trips in February") +
  scale_fill_discrete(name = "Year") +
  theme_minimal()
  

```


```{r}

# Time Series (faceted)

trips |>
  st_drop_geometry() |>
  mutate(year = year(started_at),
         month = month(started_at)) |>
  group_by(origin_name, month, year) |>
  summarise(n_trips = n()) |>
  filter(!is.na(origin_name),
         (year < 2025) & (year > 2020)) |>
  ggplot() +
  geom_line(aes(x = month, y = n_trips, color = as.factor(year))) +
  facet_wrap(~ factor(origin_name, levels = c('Shady Grove', 'Rockville', 'Twinbrook', 'White Flint', 'Grosvenor-Strathmore',
                                              'Medical Center', 'Bethesda', 'Friendship Heights', 'Glenmont', 'Wheaton',
                                              'Forest Glen', 'Silver Spring', 'Takoma')), 
             scales = "free", nrow = 2, ncol = 8) +
  theme_minimal() +
  labs(x = "Month", y = "Monthly Total Number of Trips") +
  scale_color_discrete(name = "Year")

```




```{r}

wk_member <- trips |>
  st_drop_geometry() |>
  mutate(year = year(started_at),
         week = week(started_at)) |>
  group_by(member_casual, week, year) |>
  summarise(n_trips = n()) 


ggplot(data = wk_member, aes(x = member_casual, y = n_trips)) +
  stat_boxplot(geom = "errorbar", width = 0.2) +
  geom_boxplot(aes(fill = member_casual)) +
  scale_fill_hue(l = 90, c = 30) +
  labs(x = "Membership Status", y = "Weekly Total Number of Trips", fill = "Membership") +
  theme_light() +
  # theme(legend.position = "none") 
  theme_light()


t.test(n_trips ~ member_casual, data = wk_member)
  
```


```{r}

wk_dest <- trips |>
  st_drop_geometry() |>
  mutate(year = year(started_at),
         week = week(started_at)) |>
  group_by(dest_metro, week, year) |>
  summarise(n_trips = n()) 


ggplot(data = wk_dest, aes(x = dest_metro, y = n_trips)) +
  stat_boxplot(geom = "errorbar", width = 0.2) +
  geom_boxplot(aes(fill = dest_metro)) +
  scale_fill_hue(l = 90, c = 30) +
  labs(x = "Trip Destination, relative to Metro Station Radius", y = "Weekly Total Number of Trips", fill = "Destination") +
  theme_light() +
  # theme(legend.position = "none") 
  theme_light()


t.test(n_trips ~ dest_metro, data = wk_dest)

```


```{r}

wk_origin <- trips |>
  st_drop_geometry() |>
  mutate(year = year(started_at),
         week = week(started_at)) |>
  group_by(origin_metro, week, year) |>
  summarise(n_trips = n()) 


ggplot(data = wk_origin, aes(x = origin_metro, y = n_trips)) +
  stat_boxplot(geom = "errorbar", width = 0.2) +
  geom_boxplot(aes(fill = origin_metro)) +
  scale_fill_hue(l = 90, c = 30) +
  labs(x = "Trip Origin, relative to Metro Station Radius", y = "Weekly Total Number of Trips", fill = "Origin") +
  theme_light() +
  # theme(legend.position = "none") 
  theme_light()


t.test(n_trips ~ origin_metro, data = wk_origin)

```


```{r}


dest_labs <- c("Destination Near Metro", "Destination Far From Metro")
names(dest_labs) <- c("inside", "outside")

origin_labs <- c("Initiated Near Metro", "Initiated Far From Metro")
names(origin_labs) <- c("inside", "outside")

ggplot() +
  geom_histogram(data = trips[trips$duration < 20,], aes(x = duration), bins = 30) +
  facet_grid(dest_metro ~ origin_metro, labeller = labeller(dest_metro = dest_labs, origin_metro = origin_labs)) +
  theme_bw() +
  labs(y = "Count", x = "Trip Duration (minutes)")
  

```



